import os
import numpy as np
import pandas as pd

import torch
import torch.optim as optim
from torch    import nn
from torch.nn import functional as F
from torch.optim.lr_scheduler import ExponentialLR, StepLR
from torch_geometric.data     import Data
from torch_geometric.loader   import DataLoader
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score
from optparse  import OptionParser

import sys
sys.path.append("/work/s6300121/LiveTransForM-main/metal")        
from src.metal_chemutils import *

def load_config_by_version(csv_path, version):
    """
    Loads configuration parameters from a CSV file for a specific version.
    
    The CSV should have a header row with parameter names, including a "version" column.
    """
    df = pd.read_csv(csv_path)
    # DataFrame for the given version.
    config_row = df[df['version'] == version]
    if config_row.empty:
        raise ValueError(f"No configuration found for version {version}")
    config = config_row.iloc[0].to_dict()
    
    # Convert parameters to appropriate types.
    # Adjust the casting based on your parameter types.
    config['test_size']    = float(config['test_size'])
    config['num_features'] = int(config['num_features'])
    config['output_size']  = int(config['output_size'])
    config['batch_size']   = int(config['batch_size'])
    config['num_epochs']   = int(config['num_epochs'])
    config['dropout']      = float(config['dropout'])
    config['weight_decay'] = float(config['weight_decay'])
    config['depth']        = int(config['depth'])
    
    return config




class pka_Dataloader():
    def __init__(self):
        pass

    def data_loader_pka(file_path, columns, tensorize_fn, batch_size, test_size=0.2):
        """
        è¼‰å…¥å·²æ¨™è¨˜çš„pKaæ•¸æ“šç”¨æ–¼GCNæ¨¡å‹è¨“ç·´
        
        Args:
            file_path: CSVæª”æ¡ˆè·¯å¾‘ï¼ŒåŒ…å«SMILESå’ŒpKaæ¨™è¨˜è³‡è¨Š
            columns: éœ€è¦çš„CSVæ¬„ä½æ¸…å–®
            tensorize_fn: å°‡SMILESè½‰æ›ç‚ºåœ–å½¢çš„å‡½æ•¸
            batch_size: æ‰¹æ¬¡å¤§å°
            test_size: æ¸¬è©¦é›†å¤§å°æ¯”ä¾‹
            
        Returns:
            è¨“ç·´è³‡æ–™é›†å’Œæ¸¬è©¦è³‡æ–™è¼‰å…¥å™¨
        """
        df = pd.read_csv(file_path)
        df = df[columns]
        
        # åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
        train_data, test_data = train_test_split(df, test_size=test_size, random_state=42)
        
        def tensorize_dataset(data):
            dataset = []
            for _, row in data.iterrows():
                try:
                    # å–å¾—åˆ†å­SMILESå’ŒpKaçŸ©é™£ä¿¡æ¯
                    smiles = row["smiles"]
                    # smiles = "Nc1ccncc1[N+]([O-])=O"
                    # pka_matrix_str = "[(4, 5.05)]"
                    # mol_tensor = tensorize_for_pka(smiles)
                    # fatoms, graphs, edge_features = mol_tensor[0], mol_tensor[1], mol_tensor[2]
                    
                    # å°‡pKaçŸ©é™£å¾å­—ç¬¦ä¸²è½‰æ›å›åˆ—è¡¨æ ¼å¼
                    # æ ¼å¼å¦‚ï¼š[(0, 9.16), (4, 10.82)]
                    pka_matrix_str = row["pka_matrix"]
                    pka_matrix = eval(pka_matrix_str)
                    
                    # ä½¿ç”¨tensorize_fnå°‡SMILESè½‰æ›ç‚ºåœ–å½¢è¡¨ç¤º
                    mol_tensor = tensorize_fn([smiles])
                    fatoms, graphs, edge_features = mol_tensor[0], mol_tensor[1], mol_tensor[2]
                    
                    # å‰µå»ºåŸå­ç´šåˆ¥çš„pKaæ¨™ç±¤
                    # åˆå§‹åŒ–æ‰€æœ‰åŸå­çš„pKaæ¨™ç±¤ç‚º0
                    num_atoms = fatoms.size(0)
                    atom_pka_labels = torch.zeros(num_atoms)
                    
                    # å°æœ‰pKaå€¼çš„åŸå­é€²è¡Œæ¨™è¨˜
                    for atom_idx, pka_value in pka_matrix:
                        atom_pka_labels[atom_idx] = pka_value
                    
                    # è¨ˆç®—pKaå€¼çš„æ•¸é‡
                    pka_count = len(pka_matrix)
                    
                    metal_ion = row['metal_ion']
                    # å‰µå»ºDataå°è±¡
                    data_item = Data(
                        x=fatoms,
                        edge_index=graphs,
                        edge_attr=edge_features, 
                        pka_labels=atom_pka_labels,  # æ¯å€‹åŸå­çš„pKaæ¨™ç±¤
                        pka_count=pka_count,         # pKaå€¼çš„æ•¸é‡
                        metal_ion=metal_ion,         # é‡‘å±¬é›¢å­
                        smiles=smiles,
                    )
                    
                    dataset.append(data_item)
                except Exception as e:
                    print(f"è™•ç†è³‡æ–™æ™‚å‡ºéŒ¯: {e} (SMILES: {smiles})")
                    continue
                    
            return dataset
        
        # å‰µå»ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†
        train_dataset = tensorize_dataset(train_data)
        test_dataset = tensorize_dataset(test_data)
        
        # å‰µå»ºè³‡æ–™åŠ è¼‰å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        return train_loader, test_loader
    
    @staticmethod
    def evaluate_pka_model(model, loader, device, output_file="", save_path=""):
        """
        å›å‚³:  avg_loss, avg_cla_loss, avg_reg_loss, metrics(dict)
        metrics å…§å«
            accuracy / precision / recall / f1 / pka_atom_accuracy
            rmse_gt   â€“ æ‰€æœ‰çœŸå¯¦ pKa åŸå­
            rmse_hit  â€“ çœŸå¯¦ä¸”æ¨¡å‹ä¹Ÿé æ¸¬ç‚ºæ­£çš„åŸå­
        """
        model.eval()
        tot_loss = tot_c = tot_r = n_batch = 0

        y_true, y_pred = [], []            # åˆ†é¡
        tgt_all, pred_all = [], []         # æ‰€æœ‰ gt pKa
        tgt_hit, pred_hit = [], []         # å‘½ä¸­æ‰ç®—

        results_data = []

        with torch.no_grad():
            for batch in loader:
                batch = batch.to(device, non_blocking=True)
                
                # è™•ç†é‡‘å±¬é›¢å­ç‰¹å¾µï¼Œèˆ‡è¨“ç·´æ™‚ä¿æŒä¸€è‡´
                if hasattr(batch, 'metal_ion'):
                    # ç‚ºæ¯å€‹åˆ†å­å‰µå»ºé‡‘å±¬ç‰¹å¾µå¼µé‡
                    batch_size = len(batch.metal_ion)
                    # å¾metal_modelså¼•å…¥
                    from src.metal_models import MetalFeatureExtractor
                    
                    # å‰µå»ºé‡‘å±¬ç‰¹å¾µåˆ—è¡¨ï¼Œç”¨æ–¼æ‰¹æ¬¡è™•ç†
                    metal_features_list = []
                    
                    # ä½¿ç”¨æ–°çš„æ–¹æ³•ç²å–é‡‘å±¬ç‰¹å¾µ
                    for ion in batch.metal_ion:
                        try:
                            # ä½¿ç”¨æ–°çš„ç‰¹å¾µæå–æ–¹æ³•ï¼Œç›´æ¥ç²å–è™•ç†å¥½çš„ç‰¹å¾µ
                            metal_feat = MetalFeatureExtractor.get_metal_features_for_training(ion, device)
                            metal_features_list.append([metal_feat, None, None])  # ä¿æŒèˆ‡è¿”å›å…ƒçµ„ç›¸åŒçš„çµæ§‹
                        except Exception as e:
                            print(f"ç²å–é‡‘å±¬ç‰¹å¾µæ™‚å‡ºéŒ¯: {e}")
                            # ä½¿ç”¨é»˜èªç‰¹å¾µ
                            metal_feat_dim = MetalFeatureExtractor.get_metal_feature_dim()
                            metal_feat = torch.zeros(metal_feat_dim, device=device)
                            metal_features_list.append([metal_feat, None, None])
                    
                    # å°‡é‡‘å±¬ç‰¹å¾µåˆ—è¡¨æ·»åŠ åˆ°batch
                    batch.metal_features = metal_features_list
                
                logits, pka_raw, (loss, loss_c, loss_r) = model(batch)

                # ------ ç´¯ç©æå¤± ------
                tot_loss += loss.item(); tot_c += loss_c.item(); tot_r += loss_r.item(); n_batch += 1

                # ------ åˆ†é¡è©•ä¼° ------
                gt_mask   = (batch.pka_labels > 0)
                pred_mask = logits.argmax(1) == 1

                y_true.append(gt_mask.cpu())
                y_pred.append(pred_mask.cpu())

                # ------ å›æ­¸è©•ä¼° ------
                # gt pKa å…¨éƒ¨
                tgt_all.append(batch.pka_labels[gt_mask].cpu())
                pred_all.append(pka_raw[gt_mask].cpu())

                # ğŸ¯ "å‘½ä¸­ï¼ˆhitï¼‰" = åŒæ™‚è¢«æ­£ç¢ºåˆ†é¡ç‚ºæœ‰ pKa ä¸¦ä¸”è©²åŸå­çš„çœŸå¯¦ label æ˜¯æœ‰ pKa çš„
                hit_mask = gt_mask & pred_mask
                if hit_mask.any():
                    tgt_hit.append(batch.pka_labels[hit_mask].cpu())
                    pred_hit.append(pka_raw[hit_mask].cpu())

                # ------ (å¯é¸) ä¿å­˜ per-molecule çµæœ ------
                if output_file:
                    smiles = batch.smiles
                    idx_gt = torch.nonzero(gt_mask).squeeze(1).cpu().tolist()
                    true_v = batch.pka_labels[gt_mask].cpu().tolist()
                    pred_v = pka_raw[gt_mask].cpu().tolist()
                    results_data.append({
                        "smiles": smiles,
                        "gt_idx": idx_gt,
                        "gt_pka": true_v,
                        "pred_pka": pred_v
                    })

        # --------- èšåˆ ---------
        avg_loss = tot_loss / n_batch
        avg_cla  = tot_c    / n_batch
        avg_reg  = tot_r    / n_batch

        y_true = torch.cat(y_true).numpy()
        y_pred = torch.cat(y_pred).numpy()

        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
        acc     = accuracy_score(y_true, y_pred)
        prec    = precision_score(y_true, y_pred, zero_division=0)
        rec     = recall_score(y_true, y_pred, zero_division=0)
        f1      = f1_score(y_true, y_pred, zero_division=0)
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()
        pka_atom_acc = rec   # ï¼ TP / (TP+FN)

        # å›æ­¸ RMSE
        import numpy as np, math
        rmse_gt  = math.sqrt(np.mean((torch.cat(pred_all).numpy() -
                                    torch.cat(tgt_all ).numpy())**2)) if tgt_all else 0.0
        rmse_hit = (math.sqrt(np.mean((torch.cat(pred_hit).numpy() -
                                    torch.cat(tgt_hit).numpy())**2))
                    if tgt_hit else 0.0)

        print(f"é æ¸¬æº–ç¢ºåº¦: {rec:.3f}  |  "
            f"RMSE(gt)={rmse_gt:.3f}  RMSE(hit)={rmse_hit:.3f}")
        metrics = dict(
            accuracy=acc, precision=prec, recall=rec, f1=f1,
            pka_atom_accuracy=pka_atom_acc,
            rmse_gt=rmse_gt, rmse_hit=rmse_hit,
            confusion_matrix=dict(tn=int(tn), fp=int(fp), fn=int(fn), tp=int(tp))
        )

        # --------- å„²å­˜ CSV ---------
        if output_file and results_data:
            os.makedirs(save_path, exist_ok=True)
            df = pd.DataFrame(results_data)
            df.to_csv(os.path.join(save_path, f"pka-{output_file}"), index=False)
            print(f"çµæœå·²å­˜ {os.path.join(save_path, f'pka-{output_file}')}")

        return avg_loss, avg_cla, avg_reg, metrics

